{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b67b9a3",
   "metadata": {},
   "source": [
    "# PWBI Admin API Data Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27548d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urljoin\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b39180",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae5332",
   "metadata": {},
   "source": [
    "## Key Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05edbd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_key_globals(show_display=False):\n",
    "    global access_token, base_url, header\n",
    "    credential = InteractiveBrowserCredential()\n",
    "    credential.authenticate()\n",
    "    api = 'https://analysis.windows.net/powerbi/api/.default'\n",
    "    access_token = credential.get_token(api).token\n",
    "    base_url = 'https://api.powerbi.com/v1.0/myorg/'\n",
    "    header = {'Authorization': f'Bearer {access_token}'}\n",
    "    if show_display:\n",
    "        print(f\"Updated Access Token \\n{access_token}\")\n",
    "    return header\n",
    "\n",
    "def make_api_call(url, params, headers):\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    if response.status_code == 403:\n",
    "        headers = update_key_globals()\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "    elif response.status_code == 404:\n",
    "        response = None\n",
    "    return response\n",
    "    \n",
    "def single_api_call(path, params, record_path='value'):\n",
    "    file_name = path.split('/')[-1]\n",
    "    url = urljoin(base_url, path)\n",
    "    response = make_api_call(url, params, header)\n",
    "    if response == None:\n",
    "        return pd.DataFrame()\n",
    "    elif response.status_code == 200:\n",
    "        df = pd.json_normalize(response.json(), record_path = record_path)\n",
    "        df.to_csv(f'data/{file_name}.csv')\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} at {response.url} with message \\n{response.json()}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e493f5",
   "metadata": {},
   "source": [
    "## Activity API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d96355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activity_json_to_df(response, record_path='activityEventEntities'):\n",
    "    response_json = response.json()\n",
    "    df = pd.json_normalize(response_json, record_path=record_path)\n",
    "    continuation_uri = response_json['continuationUri']\n",
    "    last_result_set = response_json['lastResultSet']\n",
    "    return last_result_set, continuation_uri, df\n",
    "    \n",
    "\n",
    "def get_view_report_activities(start_date='2023-11-18',end_date='2023-12-17'):\n",
    "\n",
    "    # Convert to datetime objects\n",
    "    start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    delta = timedelta(hours=23, minutes=59, seconds=59, milliseconds=999)\n",
    "\n",
    "    # Generate dates\n",
    "    start_dates = []\n",
    "    end_dates = []\n",
    "    current = start\n",
    "    while current <= end:\n",
    "        start_dates.append(current.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\"))\n",
    "        end_dates.append((current + delta).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\"))\n",
    "        current += timedelta(days=1)\n",
    "        \n",
    "\n",
    "    # Calling the API\n",
    "    path = 'admin/activityevents'\n",
    "    url = urljoin(base_url, path)\n",
    "    \n",
    "    # Initialize the final activity dataframe\n",
    "    view_activity_df = []\n",
    "    \n",
    "    for start, end in tqdm(zip(start_dates, end_dates), total=len(start_dates)):\n",
    "\n",
    "        # Initializing Parameters for the API Call\n",
    "        params = {\n",
    "            'startDateTime': f\"'{start}'\",\n",
    "            'endDateTime': f\"'{end}'\",\n",
    "            '$filter': \"Activity eq 'viewreport'\"\n",
    "        }\n",
    "        \n",
    "        # Calling the Final API URL\n",
    "        response = make_api_call(url, params, header)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            last_result_set, continuation_uri, first_df = activity_json_to_df(response)\n",
    "            date_lvl_data = [first_df]\n",
    "            \n",
    "            while not last_result_set:\n",
    "                response = make_api_call(continuation_uri, {}, header)\n",
    "                if response.status_code == 200:\n",
    "                    last_result_set, continuation_uri, continuation_df = activity_json_to_df(response)\n",
    "                    date_lvl_data.append(continuation_df)\n",
    "                else:\n",
    "                    print(f\"Error: {response.status_code} at {response.url} with \\n{response.json()}\")\n",
    "                    break\n",
    "\n",
    "            view_activity_df.extend(date_lvl_data)\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} at {response.url} with \\n{response.json()}\")\n",
    "            break\n",
    "    \n",
    "    return pd.concat(view_activity_df, axis=0) if view_activity_df else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e76e23",
   "metadata": {},
   "source": [
    "### Activity Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "255ae399",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'headers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Windows\\Temp\\ipykernel_23844\\761527362.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mupdate_key_globals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Windows\\Temp\\ipykernel_23844\\2175034891.py\u001b[0m in \u001b[0;36mupdate_key_globals\u001b[1;34m(show_display)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshow_display\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Updated Access Token \\n{access_token}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_api_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'headers' is not defined"
     ]
    }
   ],
   "source": [
    "update_key_globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b8ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the API Function\n",
    "start_date='2023-12-06'\n",
    "end_date='2024-01-06'\n",
    "\n",
    "if os.path.isfile(f\"data/reportViewActivity{start_date}_{end_date}.csv\"):\n",
    "    df_view_activity = pd.read_csv(f\"data/reportViewActivity{start_date}_{end_date}.csv\")\n",
    "else:\n",
    "    df_view_activity = get_view_report_activities(start_date, end_date)\n",
    "    df_view_activity.to_csv(f\"data/reportViewActivity{start_date}_{end_date}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing BI team members\n",
    "bi_team_members = [\n",
    "    'lakshmi.n@gagroup.net', 'pooja.s@gagroup.net', 'Prajwal.G@gagroup.net', \n",
    "    'Sachin.bm@gagroup.net', 'elizabeth@gagroup.net', 'aravindhakshan.r@gagroup.net'\n",
    "]\n",
    "\n",
    "df_view_activity = df_view_activity[~(df_view_activity.UserId.isin(bi_team_members))].copy().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b34bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure CreationTime is a datetime object\n",
    "df_view_activity['CreationTime'] = pd.to_datetime(df_view_activity['CreationTime'])\n",
    "\n",
    "# Convert the datetime to timezone-aware (assuming original is UTC)\n",
    "df_view_activity['CreationTime'] = df_view_activity['CreationTime'].dt.tz_localize('UTC')\n",
    "\n",
    "# Now convert to 'Asia/Dubai' timezone\n",
    "df_view_activity['CreationTimeLocal'] = df_view_activity['CreationTime'].dt.tz_convert('Asia/Dubai')\n",
    "\n",
    "# Extracting date and time\n",
    "df_view_activity['ActivityDate'] = df_view_activity['CreationTimeLocal'].dt.date\n",
    "df_view_activity['ActivityTime'] = df_view_activity['CreationTimeLocal'].dt.time\n",
    "df_view_activity['Activityhour'] = df_view_activity['CreationTimeLocal'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b23911",
   "metadata": {},
   "source": [
    "## Workspace Expanded Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'admin/groups'\n",
    "params = {\n",
    "    '$expand': 'reports',\n",
    "    '$top': 1000\n",
    "}\n",
    "\n",
    "url = urljoin(base_url, path)\n",
    "response = make_api_call(url, params, header)\n",
    "if response == None:\n",
    "    df_groups_e = pd.DataFrame()\n",
    "elif response.status_code == 200:\n",
    "    json_groups_e = response.json()['value']\n",
    "    df_groups_e = pd.json_normalize(\n",
    "        json_groups_e,\n",
    "        record_path='reports',\n",
    "        meta=[\n",
    "            'id','isOnDedicatedCapacity', 'capacityMigrationStatus',\n",
    "            'type','state','name','capacityId','defaultDatasetStorageFormat'\n",
    "        ],\n",
    "        errors='ignore',\n",
    "        meta_prefix='workspace_'\n",
    ")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} at {response.url} with message \\n{response.json()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd26002",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups_e = df_groups_e[\n",
    "    [\n",
    "        'workspace_id','workspace_name','workspace_type','workspace_state',\n",
    "        'workspace_isOnDedicatedCapacity','workspace_capacityMigrationStatus',\n",
    "        'workspace_capacityId','workspace_defaultDatasetStorageFormat',   \n",
    "        'id', 'name', 'datasetId', 'appId', 'createdDateTime', 'modifiedDateTime'\n",
    "    ]\n",
    "].copy()\n",
    "df_groups_e = df_groups_e[df_groups_e['workspace_type'] == 'Workspace'].copy()\n",
    "df_groups_e.rename(\n",
    "    {\n",
    "        'id': 'report_id',\n",
    "        'name': 'report_name',\n",
    "        'createdDateTime': 'reportCreatedDatetime',\n",
    "        'modifiedDateTime': 'reportModifiedDatetime'\n",
    "    },\n",
    "    inplace=True,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"Shape of groups = {df_groups_e.shape}\")\n",
    "df_groups_e.to_csv('data/groups_e.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059bd79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Active Groups only\n",
    "df_active_groups = df_groups_e[df_groups_e['workspace_state']=='Active'].copy()\n",
    "print(f\"Shape of active groups = {df_active_groups.shape}\")\n",
    "df_active_groups.to_csv('data/active_groups_e.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfff4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enriched with app data\n",
    "df_active_groups_2 = pd.merge(\n",
    "    left = df_active_groups[df_active_groups['appId'].isna()==True],\n",
    "    right = df_active_groups[df_active_groups['appId'].isna()==False][\n",
    "        [\n",
    "            'workspace_id','datasetId','report_id','report_name','appId','reportCreatedDatetime','reportModifiedDatetime'\n",
    "        ]\n",
    "    ],\n",
    "    how = 'left',\n",
    "    left_on = ['workspace_id','datasetId'],\n",
    "    right_on = ['workspace_id','datasetId']\n",
    ")\n",
    "\n",
    "df_active_groups_2 = df_active_groups_2[\n",
    "    [\n",
    "        'workspace_id', 'workspace_name', 'workspace_type', 'workspace_state',\n",
    "        'workspace_isOnDedicatedCapacity', 'report_id_x', 'report_name_x', \n",
    "        'datasetId','reportCreatedDatetime_x', 'reportModifiedDatetime_x', \n",
    "        'report_id_y', 'report_name_y', 'appId_y', 'reportCreatedDatetime_y',\n",
    "        'reportModifiedDatetime_y'\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "df_active_groups_2.rename(\n",
    "    {\n",
    "        'report_id_x': 'report_id',\n",
    "        'report_name_x': 'report_name',\n",
    "        'reportCreatedDatetime_x': 'reportCreatedDatetime',\n",
    "        'reportModifiedDatetime_x': 'reportModifiedDatetime',\n",
    "        'report_id_y': 'app_report_id',\n",
    "        'report_name_y': 'app_report_name',\n",
    "        'reportCreatedDatetime_y': 'app_reportCreatedDatetime',\n",
    "        'reportModifiedDatetime_y': 'app_reportModifiedDatetime',\n",
    "        'appId_y': 'appId'\n",
    "    },\n",
    "    axis=1,\n",
    "    inplace=True\n",
    "\n",
    ")\n",
    "\n",
    "# Fill NaNs to carry out str operations\n",
    "df_active_groups_2['app_report_name'].fillna('Not Applicable', inplace=True)\n",
    "\n",
    "# Itentify duplicate matches\n",
    "df_active_groups_2['duplicate_match_flag'] = df_active_groups_2.apply(lambda x: 1 if ((x['report_name'].strip() in x['app_report_name'].strip()) or (x['app_report_name'] == 'Not Applicable')) else 0, axis=1)\n",
    "\n",
    "print(df_active_groups_2.shape)\n",
    "df_active_groups_2.to_csv('data/active_groups_e_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d733d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_report_views = df_view_activity.groupby('ReportId').agg(\n",
    "    {\n",
    "        'UserKey': [('reportUserCount', lambda x: x.nunique()), ('viewerCount', 'count')],\n",
    "        'CreationTime': [('latestAccess','min')]\n",
    "    }\n",
    "\n",
    ").reset_index()\n",
    "\n",
    "# Flatten the MultiIndex in columns\n",
    "df_report_views.columns = ['_'.join(col).strip() if type(col) is tuple else col for col in df_report_views.columns.values]\n",
    "\n",
    "df_report_views.rename(\n",
    "    {\n",
    "        'ReportId_': 'report_id',\n",
    "        'UserKey_reportUserCount' : 'reportUserCount',\n",
    "        'UserKey_viewerCount' : 'reportViewerCount',\n",
    "        'CreationTime_latestAccess': 'reportLatestAccess'\n",
    "    },\n",
    "    axis=1,\n",
    "    inplace=True\n",
    "\n",
    ")\n",
    "\n",
    "df_report_views['report_id'] = df_report_views['report_id'].str.upper()\n",
    "\n",
    "df_report_views[df_report_views['report_id'] == '0B5FBBFC-8F31-4132-9A4E-39E3E5900DD8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the view table\n",
    "df_active_groups_2 = pd.merge(left=df_active_groups_2, right=df_report_views, how='left', left_on='report_id', right_on='report_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a6d8a",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a80b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get Datasets\n",
    "path = 'admin/datasets'\n",
    "params = {}\n",
    "\n",
    "if 'df_datasets' not in globals():\n",
    "    try:\n",
    "        df_datasets = pd.read_csv(f\"data/{path.split('/')[-1]}.csv\")\n",
    "    except FileNotFoundError:\n",
    "        df_datasets = single_api_call(path, params, record_path='value')  \n",
    "        \n",
    "print(f\"Shape of the extracted data is {df_datasets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1cae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datasets = df_datasets[\n",
    "    [\n",
    "        'id', 'name', 'configuredBy', 'isRefreshable', 'createdDate','contentProviderType','workspaceId'\n",
    "    ]\n",
    "].rename(\n",
    "    {\n",
    "        'id': 'datasetId',\n",
    "        'name': 'datasetName',\n",
    "    },\n",
    "    axis = 1\n",
    ").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f964b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of the df_datasets = {df_datasets.shape}\")\n",
    "df_datasets.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc558abb",
   "metadata": {},
   "source": [
    "### Refresh Schedule\n",
    "Getting the refresh schedule of the datasets that are refreshable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every refreshable, get the refresh schedule\n",
    "df_refresh_schedule = pd.DataFrame()\n",
    "for dataset in tqdm(df_datasets[df_datasets['isRefreshable'] == True]['datasetId']): \n",
    "    path = f'datasets/{dataset}/refreshSchedule'\n",
    "    params = {}\n",
    "    inner_df = single_api_call(path, params, record_path=None)\n",
    "    inner_df['datasetId'] = dataset\n",
    "    df_refresh_schedule = pd.concat([df_refresh_schedule,inner_df], axis=0)\n",
    "\n",
    "# Converting the list stored in the dataframe to csv (Prerequisite for the next step)\n",
    "df_refresh_schedule.days = df_refresh_schedule.days.apply(lambda x: ','.join(x))\n",
    "df_refresh_schedule.times = df_refresh_schedule.times.apply(lambda x: ','.join(x))\n",
    "\n",
    "# Dropping duplicates\n",
    "df_refresh_schedule.drop_duplicates(inplace=True)\n",
    "\n",
    "# Creating a persistent copy\n",
    "df_refresh_schedule.to_csv('data/refresh_schedule.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad62bec1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_datasets = pd.merge(left=df_datasets, right=df_refresh_schedule[['datasetId', 'times', 'days','enabled','localTimeZoneId']], how='left', left_on='datasetId', right_on='datasetId')\n",
    "df_datasets = df_datasets[\n",
    "    [\n",
    "        'datasetId', 'datasetName', 'configuredBy', 'isRefreshable', 'createdDate',\n",
    "        'contentProviderType','workspaceId','times', 'days', 'enabled', 'localTimeZoneId'\n",
    "    ]\n",
    "].rename(\n",
    "    {\n",
    "        'times': 'refreshTimes',\n",
    "        'days': 'refreshDays',\n",
    "        'enabled': 'refreshEnabled',\n",
    "    }, \n",
    "    axis='columns'\n",
    ").copy()\n",
    "print(f\"Shape of the updated reports dataframe is {df_datasets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd811d9",
   "metadata": {},
   "source": [
    "### Refresh History\n",
    "Getting the refresh history of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d074f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every refreshable, get the refresh schedule\n",
    "df_refresh_history = pd.DataFrame()\n",
    "for dataset in tqdm(df_datasets[df_datasets['isRefreshable'] == True]['datasetId']):\n",
    "    path = f'datasets/{dataset}/refreshes'\n",
    "    params = {}\n",
    "    inner_df = single_api_call(path, params, record_path='value')\n",
    "    inner_df['datasetId'] = dataset\n",
    "    df_refresh_history = pd.concat([df_refresh_history,inner_df], axis=0)\n",
    "    \n",
    "df_refresh_history['startTime'] = pd.to_datetime(df_refresh_history['startTime']).dt.tz_convert('Asia/Dubai')\n",
    "df_refresh_history['endTime'] = pd.to_datetime(df_refresh_history['endTime']).dt.tz_convert('Asia/Dubai')\n",
    "df_refresh_history['refreshDuration'] = df_refresh_history['endTime']-df_refresh_history['startTime']\n",
    "\n",
    "print(f\"Shape of refresh history df {df_refresh_history.shape}\")\n",
    "df_refresh_history.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cfaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_refresh_stats = df_refresh_history.groupby('datasetId').agg({\n",
    "    'startTime': 'max',\n",
    "    'refreshDuration': 'mean'\n",
    "}).reset_index().rename(\n",
    "    {\n",
    "        'startTime': 'latestRefreshDate',\n",
    "        'refreshDuration': 'avgRefreshDuration'\n",
    "    }, axis=1)\n",
    "df_dataset_refresh_stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126db8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_datasets = pd.merge(left=df_datasets, right=df_dataset_refresh_stats, how='left', left_on='datasetId', right_on='datasetId')\n",
    "df_datasets = df_datasets[\n",
    "    [\n",
    "        'datasetId', 'datasetName', 'configuredBy', 'isRefreshable', 'createdDate',\n",
    "        'contentProviderType','workspaceId','refreshTimes', 'refreshDays', 'refreshEnabled', \n",
    "        'localTimeZoneId', 'latestRefreshDate', 'avgRefreshDuration'\n",
    "    ]\n",
    "].copy() \n",
    "df_datasets['avgRefreshDuration'] = df_datasets['avgRefreshDuration'].dt.seconds/60\n",
    "print(f\"Shape of the updated reports dataframe is {df_datasets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36318905",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating fin\n",
    "df_active_groups2 = pd.merge(left=df_active_groups_2, right=df_datasets, how='left', left_on='datasetId', right_on='datasetId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_active_groups2 = df_active_groups2[\n",
    "    [\n",
    "        'workspace_id', 'workspace_name', 'workspace_type', 'workspace_state','workspace_isOnDedicatedCapacity', \n",
    "        'report_id', 'report_name','reportCreatedDatetime', 'reportModifiedDatetime',\n",
    "        'reportUserCount', 'reportViewerCount','reportLatestAccess',\n",
    "        'app_report_id', 'app_report_name', 'appId','app_reportCreatedDatetime', 'app_reportModifiedDatetime',\n",
    "        'duplicate_match_flag',\n",
    "        'datasetId','datasetName', 'configuredBy', 'isRefreshable', 'createdDate', 'contentProviderType', 'refreshTimes',\n",
    "        'refreshDays', 'refreshEnabled', 'localTimeZoneId', 'latestRefreshDate','avgRefreshDuration', \n",
    "    ]\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0d785",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745aa554",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Total Number of active workspaces\n",
    "active_workspaces = df_view_activity['WorkspaceId'].nunique()\n",
    "\n",
    "# Total Number of active artifacts\n",
    "active_artifacts = df_view_activity['ArtifactId'].nunique()\n",
    "\n",
    "# Total Number of active Datasets\n",
    "active_datasets = df_view_activity['DatasetId'].nunique()\n",
    "\n",
    "# Total Unique Users in the last 30 days\n",
    "active_users = df_view_activity['UserKey'].nunique()\n",
    "\n",
    "# Total Number of Views in the last 30 Days\n",
    "total_views = df_view_activity['ActivityId'].nunique()\n",
    "\n",
    "# Average Views per User\n",
    "views_per_user = df_view_activity['ActivityId'].nunique()/df_view_activity['UserKey'].nunique()\n",
    "\n",
    "# Average Views per User per day\n",
    "views_per_user_per_day = df_view_activity['ActivityId'].nunique()/df_view_activity['UserKey'].nunique()/30\n",
    "\n",
    "# Peak Viewership Day\n",
    "peak_viewership_day = df_view_activity.groupby('ActivityDate')['UserKey'].nunique().idxmax()\n",
    "\n",
    "# Peak Viewship Hour\n",
    "peak_viewership_hour = df_view_activity.groupby('Activityhour')['UserKey'].nunique().idxmax()\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(15,7))\n",
    "\n",
    "\n",
    "def create_kpi_box(ax, title, value, color):\n",
    "    # Use the entire axes to draw the rectangle\n",
    "    ax.add_patch(Rectangle((0, 0), 1, 1, color=color, alpha=0.3, transform=ax.transAxes))\n",
    "\n",
    "    # Add text\n",
    "    ax.text(0.5, 0.6, title, ha='center', va='center', fontsize=12, fontweight='bold', transform=ax.transAxes)\n",
    "    ax.text(0.5, 0.4, value, ha='center', va='center', fontsize=14, transform=ax.transAxes)\n",
    "\n",
    "    # Remove the x and y ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Creating KPI boxes\n",
    "create_kpi_box(ax[0,0], 'Active Artifacts', f'{active_artifacts}', 'silver')\n",
    "create_kpi_box(ax[0,1], 'Active Datasets', f'{active_datasets}', 'silver')\n",
    "create_kpi_box(ax[0,2], 'Active Workspaces', f'{active_workspaces}', 'silver')\n",
    "create_kpi_box(ax[1,0], 'Active Viewers', f'{active_users}', 'silver')\n",
    "create_kpi_box(ax[1,1], 'Total Views', f'{total_views}', 'silver')\n",
    "create_kpi_box(ax[1,2], 'Peak Viewership \\nDay', f'{peak_viewership_day:%Y-%m-%d}', 'silver')\n",
    "create_kpi_box(ax[2,0], 'Views per user', f'{views_per_user}', 'silver')\n",
    "create_kpi_box(ax[2,2], 'Peak Viewership \\nHour', f'{peak_viewership_hour}', 'silver')\n",
    "create_kpi_box(ax[2,1], 'Views per user per day', f'{views_per_user_per_day}', 'silver')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a397c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Viewer Trend\n",
    "view_trend = df_view_activity.groupby('ActivityDate').agg({\n",
    "    'UserKey': lambda x:x.nunique(),\n",
    "    'ActivityId': lambda x:x.nunique()\n",
    "}).reset_index().set_index('ActivityDate')\n",
    "\n",
    "# Converting index to datetime\n",
    "view_trend.index = pd.to_datetime(view_trend.index)\n",
    "\n",
    "# Identify weekends\n",
    "weekends = view_trend.index.weekday >= 5\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')  # Example of setting a style\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4), layout='constrained')\n",
    "\n",
    "# Bar chart\n",
    "# Coloring weekends differently\n",
    "ax.bar(view_trend.index[weekends], view_trend.loc[weekends, 'UserKey'], label='Viewers - Weekend', color='blue')\n",
    "ax.bar(view_trend.index[~weekends], view_trend.loc[~weekends, 'UserKey'], label='Viewers - Weekday', color='green')\n",
    "\n",
    "\n",
    "# Line plot with a secondary axis\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(view_trend.index, view_trend['ActivityId'], label='Views', color='salmon', linestyle='--', linewidth=2)\n",
    "\n",
    "# Enhancements\n",
    "ax.set_title(\"Overall Viewing Trend\", fontsize=14)\n",
    "ax.set_xlabel(\"Date\", fontsize=12)\n",
    "ax.set_ylabel(\"Unique Viewers\", fontsize=12)\n",
    "ax2.set_ylabel(\"Unique Views\", fontsize=12)\n",
    "\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # Format date\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "ax.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df8d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average viewership\n",
    "view_trend['weekends'] = view_trend.index.weekday >= 5\n",
    "\n",
    "print(f\"Average Views: \\n{view_trend.groupby('weekends')['ActivityId'].mean()}\")\n",
    "print(f\"Average Viewers: \\n{view_trend.groupby('weekends')['UserKey'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e0234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_domain(email):\n",
    "    \"\"\"\n",
    "    Function to extract the domain from an email address.\n",
    "\n",
    "    Parameters:\n",
    "    email (str): The email address from which to extract the domain.\n",
    "\n",
    "    Returns:\n",
    "    str: The extracted domain.\n",
    "    \"\"\"\n",
    "    match = re.search(r'(?<=@)[^>]+', email)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return None  # Return None or an appropriate value if the domain isn't found\n",
    "\n",
    "\n",
    "#Top 20 Users\n",
    "\n",
    "viewer_trend = df_view_activity.groupby(['UserKey','UserId']).agg({\n",
    "    'ActivityId': lambda x:x.nunique()\n",
    "}).reset_index().sort_values('ActivityId', ascending=False)\n",
    "\n",
    "viewer_trend['domain'] = viewer_trend['UserId'].apply(extract_domain)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.bar(viewer_trend['UserId'].head(20), viewer_trend['ActivityId'].head(20), width=1, edgecolor=\"white\", linewidth=0.7)\n",
    "\n",
    "plt.xticks(rotation=90) \n",
    "plt.tick_params(axis='x', which='major', labelsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer_trend.groupby('domain').head(5).sort_values('domain')[['UserId', 'ActivityId', 'domain']].to_csv('user_trend.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f4563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Top 3 overall\n",
    "df_active_groups2.sort_values('reportUserCount', ascending=False).head(20)[['workspace_name', 'report_name', 'reportUserCount', 'reportViewerCount']].sort_values('workspace_name')\n",
    "\n",
    "# Top 3 across the workspaces\n",
    "# df_active_groups2.sort_values('reportUserCount', ascending=False).groupby('workspace_name').head(3)[['workspace_name', 'report_name', 'reportUserCount', 'reportViewerCount']].sort_values('workspace_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ab8bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_active_groups2.to_csv('data/df_active_groups.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c00830",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd21c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old logic for updating access_token globals\n",
    "credential = InteractiveBrowserCredential()\n",
    "credential.authenticate()\n",
    "\n",
    "api = 'https://analysis.windows.net/powerbi/api/.default'\n",
    "access_token = credential.get_token(api)\n",
    "access_token = access_token.token\n",
    "\n",
    "access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://api.powerbi.com/v1.0/myorg/'\n",
    "header = {'Authorization': f'Bearer {access_token}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old logic for activities\n",
    "path = 'admin/activityevents'\n",
    "url = urljoin(base_url, path)\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "for each in tqdm(zip(start_dates, end_dates), total=len(start_dates)):\n",
    "\n",
    "    initial_df = pd.DataFrame()\n",
    "    intermediate_df = pd.DataFrame()\n",
    "    pen_df = pd.DataFrame()\n",
    "\n",
    "    params = {\n",
    "        'startDateTime': f\"'{each[0]}'\",\n",
    "        'endDateTime': f\"'{each[-1]}'\",\n",
    "        '$filter': \"Activity eq 'viewreport'\"\n",
    "    }\n",
    "\n",
    "    initial_response = requests.get(url, params=params, headers=header)\n",
    "\n",
    "    if initial_response.status_code == 200:\n",
    "        initial_df = pd.json_normalize(initial_response.json(), record_path='activityEventEntities')\n",
    "        continuation_uri = initial_response.json()['continuationUri']\n",
    "        last_result_set = initial_response.json()['lastResultSet']\n",
    "        while last_result_set == False:\n",
    "            intermediate_response = requests.get(continuation_uri, headers=header)\n",
    "            if intermediate_response.status_code == 200:\n",
    "                intermediate_df = pd.json_normalize(intermediate_response.json(), record_path='activityEventEntities')\n",
    "                pen_df = pd.concat([pen_df,intermediate_df ], axis = 0)\n",
    "                continuation_uri = intermediate_response.json()['continuationUri']\n",
    "                last_result_set = intermediate_response.json()['lastResultSet']\n",
    "            else:\n",
    "                print(intermediate_response.status_code)\n",
    "                break\n",
    "        final_df = pd.concat([final_df,initial_df,pen_df], axis=0)\n",
    "    else:\n",
    "        print(initial_response.status_code)\n",
    "        print(initial_response.url)\n",
    "        print(initial_response.json)\n",
    "        break \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpnb",
   "language": "python",
   "name": "jpnb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
